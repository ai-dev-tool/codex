import type { ReviewDecision } from "./review.js";
import type { ApplyPatchCommand, ApprovalPolicy } from "../../approvals.js";
import type { AppConfig } from "../config.js";
import type {
  ResponseFunctionToolCall,
  ResponseInputItem,
  ResponseItem,
} from "openai/resources/responses/responses.mjs";
import type { Reasoning } from "openai/resources.mjs";

import { log, isLoggingEnabled } from "./log.js";
import { OPENAI_BASE_URL, OPENAI_TIMEOUT_MS } from "../config.js";
import { parseToolCallArguments } from "../parsers.js";
import {
  ORIGIN,
  CLI_VERSION,
  getSessionId,
  setCurrentModel,
  setSessionId,
} from "../session.js";
import { handleExecCommand } from "./handle-exec-command.js";
import { randomUUID } from "node:crypto";
import OpenAI, { APIConnectionTimeoutError } from "openai";

// Wait time before retrying after rate limit errors (ms).
// åœ¨é‡åˆ°é€Ÿç‡é™åˆ¶é”™è¯¯åé‡è¯•å‰çš„ç­‰å¾…æ—¶é—´(æ¯«ç§’)ã€‚
const RATE_LIMIT_RETRY_WAIT_MS = parseInt(
  process.env["OPENAI_RATE_LIMIT_RETRY_WAIT_MS"] || "2500",
  10,
);

export type CommandConfirmation = {
  /** å®¡æŸ¥å†³å®šçš„ç»“æœ */
  review: ReviewDecision;
  /** å¯é€‰çš„è¡¥ä¸åº”ç”¨å‘½ä»¤ */
  applyPatch?: ApplyPatchCommand | undefined;
  /** å¯é€‰çš„è‡ªå®šä¹‰æ‹’ç»æ¶ˆæ¯ */
  customDenyMessage?: string;
  /** å¯é€‰çš„è§£é‡Šè¯´æ˜ */
  explanation?: string;
};

const alreadyProcessedResponses = new Set();

type AgentLoopParams = {
  /** ä½¿ç”¨çš„AIæ¨¡å‹åç§° */
  model: string;
  /** åº”ç”¨ç¨‹åºé…ç½® */
  config?: AppConfig;
  /** ç»™AIçš„æŒ‡ä»¤ */
  instructions?: string;
  /** å‘½ä»¤å®¡æ‰¹ç­–ç•¥ */
  approvalPolicy: ApprovalPolicy;
  /** å¤„ç†å“åº”é¡¹çš„å›è°ƒå‡½æ•° */
  onItem: (item: ResponseItem) => void;
  /** å¤„ç†åŠ è½½çŠ¶æ€çš„å›è°ƒå‡½æ•° */
  onLoading: (loading: boolean) => void;

  /** Extra writable roots to use with sandbox execution. */
  /** ä¸æ²™ç®±æ‰§è¡Œä¸€èµ·ä½¿ç”¨çš„é¢å¤–å¯å†™æ ¹ç›®å½•ã€‚ */
  additionalWritableRoots: ReadonlyArray<string>;

  /** Called when the command is not auto-approved to request explicit user review. */
  /** å½“å‘½ä»¤æœªè¢«è‡ªåŠ¨æ‰¹å‡†æ—¶è°ƒç”¨ï¼Œä»¥è¯·æ±‚æ˜ç¡®çš„ç”¨æˆ·å®¡æŸ¥ã€‚ */
  getCommandConfirmation: (
    command: Array<string>,
    applyPatch: ApplyPatchCommand | undefined,
  ) => Promise<CommandConfirmation>;
  onLastResponseId: (lastResponseId: string) => void;
};

export class AgentLoop {
  private model: string;
  private instructions?: string;
  private approvalPolicy: ApprovalPolicy;
  private config: AppConfig;
  private additionalWritableRoots: ReadonlyArray<string>;

  // Using `InstanceType<typeof OpenAI>` sidesteps typing issues with the OpenAI package under
  // the TS 5+ `moduleResolution=bundler` setup. OpenAI client instance. We keep the concrete
  // type to avoid sprinkling `any` across the implementation while still allowing paths where
  // the OpenAI SDK types may not perfectly match. The `typeof OpenAI` pattern captures the
  // instance shape without resorting to `any`.
  // ä½¿ç”¨`InstanceType<typeof OpenAI>`é¿å¼€äº†åœ¨TS 5+çš„`moduleResolution=bundler`è®¾ç½®ä¸‹OpenAIåŒ…çš„ç±»å‹é—®é¢˜ã€‚
  // OpenAIå®¢æˆ·ç«¯å®ä¾‹ã€‚æˆ‘ä»¬ä¿ç•™å…·ä½“ç±»å‹ä»¥é¿å…åœ¨å®ç°ä¸­æ•£å¸ƒ`any`ï¼ŒåŒæ—¶ä»ç„¶å…è®¸paths where
  // the OpenAI SDK types may not perfectly match. The `typeof OpenAI` pattern captures the
  // instance shape without resorting to `any`.
  private oai: OpenAI;

  private onItem: (item: ResponseItem) => void;
  private onLoading: (loading: boolean) => void;
  private getCommandConfirmation: (
    command: Array<string>,
    applyPatch: ApplyPatchCommand | undefined,
  ) => Promise<CommandConfirmation>;
  private onLastResponseId: (lastResponseId: string) => void;

  /**
   * A reference to the currently active stream returned from the OpenAI
   * client. We keep this so that we can abort the request if the user decides
   * to interrupt the current task (e.g. via the escape hotâ€‘key).
   */
  /**
   * å¯¹OpenAIå®¢æˆ·ç«¯è¿”å›çš„å½“å‰æ´»åŠ¨æµçš„å¼•ç”¨ã€‚æˆ‘ä»¬ä¿ç•™è¿™ä¸ªå¼•ç”¨ï¼Œä»¥ä¾¿åœ¨ç”¨æˆ·å†³å®š
   * ä¸­æ–­å½“å‰ä»»åŠ¡æ—¶å¯ä»¥ä¸­æ­¢è¯·æ±‚ï¼ˆä¾‹å¦‚é€šè¿‡Escapeçƒ­é”®ï¼‰ã€‚
   */
  private currentStream: unknown | null = null;
  /** Incremented with every call to `run()`. Allows us to ignore stray events
   * from streams that belong to a previous run which might still be emitting
   * after the user has canceled and issued a new command. */
  /** æ¯æ¬¡è°ƒç”¨`run()`æ—¶é€’å¢ã€‚å…è®¸æˆ‘ä»¬å¿½ç•¥æ¥è‡ªå±äºå…ˆå‰è¿è¡Œçš„æµçš„æ¸¸ç¦»äº‹ä»¶ï¼Œ
   * è¿™äº›äº‹ä»¶å¯èƒ½åœ¨ç”¨æˆ·å–æ¶ˆå¹¶å‘å‡ºæ–°å‘½ä»¤åä»åœ¨å‘é€ã€‚ */
  private generation = 0;
  /** AbortController for inâ€‘progress tool calls (e.g. shell commands). */
  /** ç”¨äºæ­£åœ¨è¿›è¡Œçš„å·¥å…·è°ƒç”¨ï¼ˆä¾‹å¦‚shellå‘½ä»¤ï¼‰çš„AbortControllerã€‚ */
  private execAbortController: AbortController | null = null;
  /** Set to true when `cancel()` is called so `run()` can exit early. */
  /** å½“è°ƒç”¨`cancel()`æ—¶è®¾ç½®ä¸ºtrueï¼Œä»¥ä¾¿`run()`å¯ä»¥æå‰é€€å‡ºã€‚ */
  private canceled = false;
  /** Function calls that were emitted by the model but never answered because
   *  the user cancelled the run.  We keep the `call_id`s around so the *next*
   *  request can send a dummy `function_call_output` that satisfies the
   *  contract and prevents the
   *    400 | No tool output found for function call â€¦
   *  error from OpenAI. */
  /** ç”±æ¨¡å‹å‘å‡ºä½†ç”±äºç”¨æˆ·å–æ¶ˆè¿è¡Œè€Œä»æœªå¾—åˆ°å›ç­”çš„å‡½æ•°è°ƒç”¨ã€‚æˆ‘ä»¬ä¿ç•™`call_id`ï¼Œ
   * ä»¥ä¾¿*ä¸‹ä¸€ä¸ª*è¯·æ±‚å¯ä»¥å‘é€ä¸€ä¸ªç¬¦åˆçº¦å®šçš„è™šæ‹Ÿ`function_call_output`ï¼Œ
   * é˜²æ­¢å‡ºç°æ¥è‡ªOpenAIçš„
   *    400 | No tool output found for function call â€¦
   * é”™è¯¯ã€‚ */
  private pendingAborts: Set<string> = new Set();
  /** Set to true by `terminate()` â€“ prevents any further use of the instance. */
  /** ç”±`terminate()`è®¾ç½®ä¸ºtrue - é˜²æ­¢å®ä¾‹çš„ä»»ä½•è¿›ä¸€æ­¥ä½¿ç”¨ã€‚ */
  private terminated = false;
  /** Master abort controller â€“ fires when terminate() is invoked. */
  /** ä¸»ä¸­æ­¢æ§åˆ¶å™¨ - åœ¨è°ƒç”¨terminate()æ—¶è§¦å‘ã€‚ */
  private readonly hardAbort = new AbortController();

  /**
   * Abort the ongoing request/stream, if any. This allows callers (typically
   * the UI layer) to interrupt the current agent step so the user can issue
   * new instructions without waiting for the model to finish.
   */
  /**
   * ä¸­æ­¢æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚/æµï¼ˆå¦‚æœæœ‰ï¼‰ã€‚è¿™å…è®¸è°ƒç”¨è€…ï¼ˆé€šå¸¸æ˜¯UIå±‚ï¼‰ä¸­æ–­å½“å‰ä»£ç†æ­¥éª¤ï¼Œ
   * ä»¥ä¾¿ç”¨æˆ·å¯ä»¥å‘å‡ºæ–°æŒ‡ä»¤ï¼Œè€Œæ— éœ€ç­‰å¾…æ¨¡å‹å®Œæˆã€‚
   */
  public cancel(): void {
    if (this.terminated) {
      return;
    }

    // Reset the current stream to allow new requests
    // é‡ç½®å½“å‰æµä»¥å…è®¸æ–°è¯·æ±‚
    this.currentStream = null;
    if (isLoggingEnabled()) {
      log(
        `AgentLoop.cancel() invoked â€“ currentStream=${Boolean(
          this.currentStream,
        )} execAbortController=${Boolean(
          this.execAbortController,
        )} generation=${this.generation}`,
      );
    }
    (
      this.currentStream as { controller?: { abort?: () => void } } | null
    )?.controller?.abort?.();

    this.canceled = true;

    // Abort any in-progress tool calls
    // ä¸­æ­¢ä»»ä½•æ­£åœ¨è¿›è¡Œçš„å·¥å…·è°ƒç”¨
    this.execAbortController?.abort();

    // Create a new abort controller for future tool calls
    // ä¸ºæœªæ¥çš„å·¥å…·è°ƒç”¨åˆ›å»ºä¸€ä¸ªæ–°çš„ä¸­æ­¢æ§åˆ¶å™¨
    this.execAbortController = new AbortController();
    if (isLoggingEnabled()) {
      log("AgentLoop.cancel(): execAbortController.abort() called");
    }

    // NOTE: We intentionally do *not* clear `lastResponseId` here.  If the
    // stream produced a `function_call` before the user cancelled, OpenAI now
    // expects a corresponding `function_call_output` that must reference that
    // very same response ID.  We therefore keep the ID around so the
    // followâ€‘up request can still satisfy the contract.
    // æ³¨æ„ï¼šæˆ‘ä»¬æ•…æ„*ä¸*åœ¨æ­¤å¤„æ¸…é™¤`lastResponseId`ã€‚å¦‚æœæµåœ¨ç”¨æˆ·å–æ¶ˆä¹‹å‰
    // äº§ç”Ÿäº†`function_call`ï¼ŒOpenAIç°åœ¨æœŸæœ›ç›¸åº”çš„`function_call_output`
    // å¿…é¡»å¼•ç”¨åŒä¸€ä¸ªå“åº”IDã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¿ç•™è¯¥IDï¼Œä»¥ä¾¿åç»­è¯·æ±‚ä»ç„¶å¯ä»¥æ»¡è¶³çº¦å®šã€‚

    // If we have *not* seen any function_call IDs yet there is nothing that
    // needs to be satisfied in a followâ€‘up request.  In that case we clear
    // the stored lastResponseId so a subsequent run starts a clean turn.
    // å¦‚æœæˆ‘ä»¬å°šæœªçœ‹åˆ°ä»»ä½•function_call IDï¼Œé‚£ä¹ˆåœ¨åç»­è¯·æ±‚ä¸­æ²¡æœ‰éœ€è¦æ»¡è¶³çš„å†…å®¹ã€‚
    // åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ¸…é™¤å­˜å‚¨çš„lastResponseIdï¼Œä»¥ä¾¿åç»­è¿è¡Œå¼€å§‹ä¸€ä¸ªå¹²å‡€çš„è½®æ¬¡ã€‚
    if (this.pendingAborts.size === 0) {
      try {
        this.onLastResponseId("");
      } catch {
        /* ignore */
      }
    }

    this.onLoading(false);

    /* Inform the UI that the run was aborted by the user. */
    /* é€šçŸ¥UIè¿è¡Œè¢«ç”¨æˆ·ä¸­æ­¢ã€‚ */
    // const cancelNotice: ResponseItem = {
    //   id: `cancel-${Date.now()}`,
    //   type: "message",
    //   role: "system",
    //   content: [
    //     {
    //       type: "input_text",
    //       text: "â¹ï¸  Execution canceled by user.",
    //     },
    //   ],
    // };
    // this.onItem(cancelNotice);

    this.generation += 1;
    if (isLoggingEnabled()) {
      log(`AgentLoop.cancel(): generation bumped to ${this.generation}`);
    }
  }

  /**
   * Hardâ€‘stop the agent loop. After calling this method the instance becomes
   * unusable: any inâ€‘flight operations are aborted and subsequent invocations
   * of `run()` will throw.
   */
  /**
   * ç¡¬åœæ­¢ä»£ç†å¾ªç¯ã€‚è°ƒç”¨æ­¤æ–¹æ³•åï¼Œå®ä¾‹å˜å¾—ä¸å¯ç”¨ï¼šä»»ä½•æ­£åœ¨è¿›è¡Œçš„æ“ä½œéƒ½ä¼šè¢«ä¸­æ­¢ï¼Œ
   * éšåè°ƒç”¨`run()`å°†æŠ›å‡ºå¼‚å¸¸ã€‚
   */
  public terminate(): void {
    if (this.terminated) {
      return;
    }
    this.terminated = true;

    this.hardAbort.abort();

    this.cancel();
  }

  public sessionId: string;
  /*
   * Cumulative thinking time across this AgentLoop instance (ms).
   * Currently not used anywhere â€“ comment out to keep the strict compiler
   * happy under `noUnusedLocals`.  Restore when telemetry support lands.
   */
  /*
   * è¯¥AgentLoopå®ä¾‹çš„ç´¯è®¡æ€è€ƒæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ã€‚
   * ç›®å‰æ²¡æœ‰åœ¨ä»»ä½•åœ°æ–¹ä½¿ç”¨ - æ³¨é‡Šæ‰ä»¥ä¿æŒåœ¨`noUnusedLocals`ä¸‹ä¸¥æ ¼ç¼–è¯‘å™¨æ»¡æ„ã€‚
   * å½“é¥æµ‹æ”¯æŒå®ç°æ—¶æ¢å¤ã€‚
   */
  // private cumulativeThinkingMs = 0;
  constructor({
    model,
    instructions,
    approvalPolicy,
    // `config` used to be required.  Some unitâ€‘tests (and potentially other
    // callers) instantiate `AgentLoop` without passing it, so we make it
    // optional and fall back to sensible defaults.  This keeps the public
    // surface backwardsâ€‘compatible and prevents runtime errors like
    // "Cannot read properties of undefined (reading 'apiKey')" when accessing
    // `config.apiKey` below.
    // `config` æ›¾ç»æ˜¯å¿…éœ€çš„ã€‚ä¸€äº›å•å…ƒæµ‹è¯•ï¼ˆä»¥åŠå¯èƒ½çš„å…¶ä»–è°ƒç”¨è€…ï¼‰å®ä¾‹åŒ–`AgentLoop`
    // æ—¶æ²¡æœ‰ä¼ é€’å®ƒï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å…¶è®¾ä¸ºå¯é€‰å¹¶å›é€€åˆ°åˆç†çš„é»˜è®¤å€¼ã€‚è¿™ä¿æŒäº†å…¬å…±æ¥å£çš„
    // å‘åå…¼å®¹æ€§ï¼Œå¹¶é˜²æ­¢åœ¨ä¸‹é¢è®¿é—®`config.apiKey`æ—¶å‡ºç°è¯¸å¦‚
    // "Cannot read properties of undefined (reading 'apiKey')"ä¹‹ç±»çš„è¿è¡Œæ—¶é”™è¯¯ã€‚
    config,
    onItem,
    onLoading,
    getCommandConfirmation,
    onLastResponseId,
    additionalWritableRoots,
  }: AgentLoopParams & { config?: AppConfig }) {
    this.model = model;
    this.instructions = instructions;
    this.approvalPolicy = approvalPolicy;

    // If no `config` has been provided we derive a minimal stub so that the
    // rest of the implementation can rely on `this.config` always being a
    // defined object.  We purposefully copy over the `model` and
    // `instructions` that have already been passed explicitly so that
    // downstream consumers (e.g. telemetry) still observe the correct values.
    // å¦‚æœæ²¡æœ‰æä¾›`config`ï¼Œæˆ‘ä»¬æ´¾ç”Ÿä¸€ä¸ªæœ€å°å­˜æ ¹ï¼Œä»¥ä¾¿å®ç°çš„å…¶ä½™éƒ¨åˆ†å¯ä»¥ä¾èµ–äº
    // `this.config`å§‹ç»ˆæ˜¯ä¸€ä¸ªå·²å®šä¹‰çš„å¯¹è±¡ã€‚æˆ‘ä»¬æœ‰æ„å¤åˆ¶å·²ç»æ˜ç¡®ä¼ é€’çš„`model`å’Œ
    // `instructions`ï¼Œä»¥ä¾¿ä¸‹æ¸¸æ¶ˆè´¹è€…ï¼ˆä¾‹å¦‚é¥æµ‹ï¼‰ä»ç„¶èƒ½è§‚å¯Ÿåˆ°æ­£ç¡®çš„å€¼ã€‚
    this.config =
      config ??
      ({
        model,
        instructions: instructions ?? "",
      } as AppConfig);
    this.additionalWritableRoots = additionalWritableRoots;
    this.onItem = onItem;
    this.onLoading = onLoading;
    this.getCommandConfirmation = getCommandConfirmation;
    this.onLastResponseId = onLastResponseId;
    this.sessionId = getSessionId() || randomUUID().replaceAll("-", "");
    // Configure OpenAI client with optional timeout (ms) from environment
    // ä½¿ç”¨æ¥è‡ªç¯å¢ƒçš„å¯é€‰è¶…æ—¶ï¼ˆæ¯«ç§’ï¼‰é…ç½®OpenAIå®¢æˆ·ç«¯
    const timeoutMs = OPENAI_TIMEOUT_MS;
    const apiKey = this.config.apiKey ?? process.env["OPENAI_API_KEY"] ?? "";
    this.oai = new OpenAI({
      // The OpenAI JS SDK only requires `apiKey` when making requests against
      // the official API.  When running unitâ€‘tests we stub out all network
      // calls so an undefined key is perfectly fine.  We therefore only set
      // the property if we actually have a value to avoid triggering runtime
      // errors inside the SDK (it validates that `apiKey` is a nonâ€‘empty
      // string when the field is present).
      // OpenAI JS SDKåªæœ‰åœ¨å¯¹å®˜æ–¹APIå‘å‡ºè¯·æ±‚æ—¶æ‰éœ€è¦`apiKey`ã€‚å½“è¿è¡Œå•å…ƒæµ‹è¯•æ—¶ï¼Œ
      // æˆ‘ä»¬æ¨¡æ‹Ÿæ‰€æœ‰ç½‘ç»œè°ƒç”¨ï¼Œæ‰€ä»¥æœªå®šä¹‰çš„å¯†é’¥å®Œå…¨å¯ä»¥æ¥å—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªåœ¨å®é™…
      // æœ‰å€¼æ—¶æ‰è®¾ç½®è¯¥å±æ€§ï¼Œä»¥é¿å…åœ¨SDKå†…éƒ¨è§¦å‘è¿è¡Œæ—¶é”™è¯¯ï¼ˆå½“è¯¥å­—æ®µå­˜åœ¨æ—¶ï¼Œ
      // å®ƒä¼šéªŒè¯`apiKey`æ˜¯ä¸€ä¸ªéç©ºå­—ç¬¦ä¸²ï¼‰ã€‚
      ...(apiKey ? { apiKey } : {}),
      baseURL: OPENAI_BASE_URL,
      defaultHeaders: {
        originator: ORIGIN,
        version: CLI_VERSION,
        session_id: this.sessionId,
      },
      ...(timeoutMs !== undefined ? { timeout: timeoutMs } : {}),
    });

    setSessionId(this.sessionId);
    setCurrentModel(this.model);

    this.hardAbort = new AbortController();

    this.hardAbort.signal.addEventListener(
      "abort",
      () => this.execAbortController?.abort(),
      { once: true },
    );
  }

  private async handleFunctionCall(
    item: ResponseFunctionToolCall,
  ): Promise<Array<ResponseInputItem>> {
    // If the agent has been canceled in the meantime we should not perform any
    // additional work. Returning an empty array ensures that we neither execute
    // the requested tool call nor enqueue any followâ€‘up input items. This keeps
    // the cancellation semantics intuitive for users â€“ once they interrupt a
    // task no further actions related to that task should be taken.
    // å¦‚æœä»£ç†åœ¨æ­¤æœŸé—´å·²è¢«å–æ¶ˆï¼Œæˆ‘ä»¬ä¸åº”æ‰§è¡Œä»»ä½•é¢å¤–å·¥ä½œã€‚è¿”å›ç©ºæ•°ç»„ç¡®ä¿æˆ‘ä»¬æ—¢ä¸æ‰§è¡Œ
    // è¯·æ±‚çš„å·¥å…·è°ƒç”¨ï¼Œä¹Ÿä¸æ’é˜Ÿä»»ä½•åç»­è¾“å…¥é¡¹ã€‚è¿™ä½¿å¾—å–æ¶ˆè¯­ä¹‰å¯¹ç”¨æˆ·ç›´è§‚ - ä¸€æ—¦ä»–ä»¬
    // ä¸­æ–­ä»»åŠ¡ï¼Œå°±ä¸åº”å†é‡‡å–ä¸è¯¥ä»»åŠ¡ç›¸å…³çš„ä»»ä½•è¿›ä¸€æ­¥è¡ŒåŠ¨ã€‚
    if (this.canceled) {
      return [];
    }
    // ---------------------------------------------------------------------
    // Normalise the functionâ€‘call item into a consistent shape regardless of
    // whether it originated from the `/responses` or the `/chat/completions`
    // endpoint â€“ their JSON differs slightly.
    // ---------------------------------------------------------------------
    // ---------------------------------------------------------------------
    // å°†å‡½æ•°è°ƒç”¨é¡¹æ ‡å‡†åŒ–ä¸ºä¸€è‡´çš„å½¢çŠ¶ï¼Œæ— è®ºå®ƒæ˜¯æ¥è‡ª`/responses`è¿˜æ˜¯
    // `/chat/completions`ç«¯ç‚¹ - å®ƒä»¬çš„JSONç•¥æœ‰ä¸åŒã€‚
    // ---------------------------------------------------------------------

    const isChatStyle =
      // The chat endpoint nests function details under a `function` key.
      // We conservatively treat the presence of this field as a signal that
      // we are dealing with the chat format.
      // èŠå¤©ç«¯ç‚¹å°†å‡½æ•°è¯¦æƒ…åµŒå¥—åœ¨`function`é”®ä¸‹ã€‚
      // æˆ‘ä»¬ä¿å®ˆåœ°å°†æ­¤å­—æ®µçš„å­˜åœ¨è§†ä¸ºæˆ‘ä»¬æ­£åœ¨å¤„ç†èŠå¤©æ ¼å¼çš„ä¿¡å·ã€‚
      // eslint-disable-next-line @typescript-eslint/no-explicit-any
      (item as any).function != null;

    const name: string | undefined = isChatStyle
      ? // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (item as any).function?.name
      : // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (item as any).name;

    const rawArguments: string | undefined = isChatStyle
      ? // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (item as any).function?.arguments
      : // eslint-disable-next-line @typescript-eslint/no-explicit-any
        (item as any).arguments;

    // The OpenAI "function_call" item may have either `call_id` (responses
    // endpoint) or `id` (chat endpoint).  Prefer `call_id` if present but fall
    // back to `id` to remain compatible.
    // OpenAI "function_call"é¡¹å¯èƒ½æœ‰`call_id`ï¼ˆresponsesç«¯ç‚¹ï¼‰æˆ–`id`
    // ï¼ˆchatç«¯ç‚¹ï¼‰ã€‚å¦‚æœå­˜åœ¨`call_id`åˆ™ä¼˜å…ˆä½¿ç”¨ï¼Œä½†å›é€€åˆ°`id`ä»¥ä¿æŒå…¼å®¹æ€§ã€‚
    // eslint-disable-next-line @typescript-eslint/no-explicit-any
    const callId: string = (item as any).call_id ?? (item as any).id;

    const args = parseToolCallArguments(rawArguments ?? "{}");
    if (isLoggingEnabled()) {
      log(
        `handleFunctionCall(): name=${
          name ?? "undefined"
        } callId=${callId} args=${rawArguments}`,
      );
    }

    if (args == null) {
      const outputItem: ResponseInputItem.FunctionCallOutput = {
        type: "function_call_output",
        call_id: item.call_id,
        output: `invalid arguments: ${rawArguments}`,
      };
      return [outputItem];
    }

    const outputItem: ResponseInputItem.FunctionCallOutput = {
      type: "function_call_output",
      // `call_id` is mandatory â€“ ensure we never send `undefined` which would
      // trigger the "No tool output foundâ€¦" 400 from the API.
      // `call_id`æ˜¯å¿…éœ€çš„ - ç¡®ä¿æˆ‘ä»¬ä»ä¸å‘é€`undefined`ï¼Œå¦åˆ™ä¼šè§¦å‘
      // APIçš„"No tool output foundâ€¦" 400é”™è¯¯ã€‚
      call_id: callId,
      output: "no function found",
    };

    // We intentionally *do not* remove this `callId` from the `pendingAborts`
    // set right away.  The output produced below is only queued up for the
    // *next* request to the OpenAI API â€“ it has not been delivered yet.  If
    // the user presses ESCâ€‘ESC (i.e. invokes `cancel()`) in the small window
    // between queuing the result and the actual network call, we need to be
    // able to surface a synthetic `function_call_output` marked as
    // "aborted".  Keeping the ID in the set until the run concludes
    // successfully lets the next `run()` differentiate between an aborted
    // tool call (needs the synthetic output) and a completed one (cleared
    // below in the `flush()` helper).
    // æˆ‘ä»¬æœ‰æ„ä¸ç«‹å³ä»`pendingAborts`é›†åˆä¸­åˆ é™¤è¿™ä¸ª`callId`ã€‚ä¸‹é¢äº§ç”Ÿçš„è¾“å‡º
    // åªæ˜¯ä¸º*ä¸‹ä¸€ä¸ª*è¯·æ±‚æ’é˜Ÿåˆ°OpenAI API - å®ƒå°šæœªè¢«ä¼ é€’ã€‚å¦‚æœç”¨æˆ·åœ¨æ’é˜Ÿç»“æœ
    // å’Œå®é™…ç½‘ç»œè°ƒç”¨ä¹‹é—´çš„å°çª—å£ä¸­æŒ‰ESC-ESCï¼ˆå³è°ƒç”¨`cancel()`ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦
    // èƒ½å¤Ÿæ˜¾ç¤ºæ ‡è®°ä¸º"aborted"çš„åˆæˆ`function_call_output`ã€‚å°†IDä¿ç•™åœ¨é›†åˆä¸­
    // ç›´åˆ°è¿è¡ŒæˆåŠŸç»“æŸï¼Œå¯è®©ä¸‹ä¸€ä¸ª`run()`åŒºåˆ†è¢«ä¸­æ­¢çš„å·¥å…·è°ƒç”¨ï¼ˆéœ€è¦åˆæˆè¾“å‡ºï¼‰
    // å’Œå·²å®Œæˆçš„å·¥å…·è°ƒç”¨ï¼ˆåœ¨ä¸‹é¢çš„`flush()`åŠ©æ‰‹ä¸­æ¸…é™¤ï¼‰ã€‚

    // used to tell model to stop if needed
    // ç”¨äºåœ¨éœ€è¦æ—¶å‘Šè¯‰æ¨¡å‹åœæ­¢
    const additionalItems: Array<ResponseInputItem> = [];

    // TODO: allow arbitrary function calls (beyond shell/container.exec)
    // TODO: å…è®¸ä»»æ„å‡½æ•°è°ƒç”¨ï¼ˆé™¤äº†shell/container.execä¹‹å¤–ï¼‰
    if (name === "container.exec" || name === "shell") {
      const {
        outputText,
        metadata,
        additionalItems: additionalItemsFromExec,
      } = await handleExecCommand(
        args,
        this.config,
        this.approvalPolicy,
        this.additionalWritableRoots,
        this.getCommandConfirmation,
        this.execAbortController?.signal,
      );
      outputItem.output = JSON.stringify({ output: outputText, metadata });

      if (additionalItemsFromExec) {
        additionalItems.push(...additionalItemsFromExec);
      }
    }

    return [outputItem, ...additionalItems];
  }

  public async run(
    input: Array<ResponseInputItem>,
    previousResponseId: string = "",
  ): Promise<void> {
    // ---------------------------------------------------------------------
    // Topâ€‘level error wrapper so that known transient network issues like
    // `ERR_STREAM_PREMATURE_CLOSE` do not crash the entire CLI process.
    // Instead we surface the failure to the user as a regular systemâ€‘message
    // and terminate the current run gracefully. The calling UI can then let
    // the user retry the request if desired.
    // ---------------------------------------------------------------------
    // ---------------------------------------------------------------------
    // é¡¶çº§é”™è¯¯åŒ…è£…å™¨ï¼Œä½¿å·²çŸ¥çš„ä¸´æ—¶ç½‘ç»œé—®é¢˜ï¼ˆå¦‚`ERR_STREAM_PREMATURE_CLOSE`ï¼‰
    // ä¸ä¼šä½¿æ•´ä¸ªCLIè¿›ç¨‹å´©æºƒã€‚ç›¸åï¼Œæˆ‘ä»¬å°†å¤±è´¥ä½œä¸ºå¸¸è§„ç³»ç»Ÿæ¶ˆæ¯å‘ˆç°ç»™ç”¨æˆ·ï¼Œ
    // å¹¶ä¼˜é›…åœ°ç»ˆæ­¢å½“å‰è¿è¡Œã€‚è°ƒç”¨UIå¯ä»¥è®©ç”¨æˆ·åœ¨éœ€è¦æ—¶é‡è¯•è¯·æ±‚ã€‚
    // ---------------------------------------------------------------------

    try {
      if (this.terminated) {
        throw new Error("AgentLoop has been terminated");
      }
      // Record when we start "thinking" so we can report accurate elapsed time.
      // è®°å½•æˆ‘ä»¬å¼€å§‹"æ€è€ƒ"çš„æ—¶é—´ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥æŠ¥å‘Šå‡†ç¡®çš„ç»è¿‡æ—¶é—´ã€‚
      const thinkingStart = Date.now();
      // Bump generation so that any late events from previous runs can be
      // identified and dropped.
      // å¢åŠ ä¸–ä»£è®¡æ•°ï¼Œä»¥ä¾¿å¯ä»¥è¯†åˆ«å¹¶ä¸¢å¼ƒæ¥è‡ªå…ˆå‰è¿è¡Œçš„ä»»ä½•å»¶è¿Ÿäº‹ä»¶ã€‚
      const thisGeneration = ++this.generation;

      // Reset cancellation flag and stream for a fresh run.
      // ä¸ºæ–°çš„è¿è¡Œé‡ç½®å–æ¶ˆæ ‡å¿—å’Œæµã€‚
      this.canceled = false;
      this.currentStream = null;

      // Create a fresh AbortController for this run so that tool calls from a
      // previous run do not accidentally get signalled.
      // ä¸ºæ­¤æ¬¡è¿è¡Œåˆ›å»ºä¸€ä¸ªæ–°çš„AbortControllerï¼Œä»¥ä¾¿æ¥è‡ªå…ˆå‰è¿è¡Œçš„å·¥å…·è°ƒç”¨
      // ä¸ä¼šæ„å¤–åœ°æ”¶åˆ°ä¿¡å·ã€‚
      this.execAbortController = new AbortController();
      if (isLoggingEnabled()) {
        log(
          `AgentLoop.run(): new execAbortController created (${this.execAbortController.signal}) for generation ${this.generation}`,
        );
      }
      // NOTE: We no longer (reâ€‘)attach an `abort` listener to `hardAbort` here.
      // A single listener that forwards the `abort` to the current
      // `execAbortController` is installed once in the constructor. Reâ€‘adding a
      // new listener on every `run()` caused the same `AbortSignal` instance to
      // accumulate listeners which in turn triggered Node's
      // `MaxListenersExceededWarning` after ten invocations.
      // æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†åœ¨æ­¤å¤„ï¼ˆé‡æ–°ï¼‰é™„åŠ `abort`ç›‘å¬å™¨åˆ°`hardAbort`ã€‚
      // ä¸€ä¸ªå°†`abort`è½¬å‘åˆ°å½“å‰`execAbortController`çš„å•ä¸€ç›‘å¬å™¨åœ¨æ„é€ å‡½æ•°ä¸­
      // åªå®‰è£…ä¸€æ¬¡ã€‚åœ¨æ¯æ¬¡`run()`ä¸Šé‡æ–°æ·»åŠ æ–°çš„ç›‘å¬å™¨ä¼šå¯¼è‡´åŒä¸€ä¸ª`AbortSignal`
      // å®ä¾‹ç´¯ç§¯ç›‘å¬å™¨ï¼Œè¿™åè¿‡æ¥ä¼šåœ¨åæ¬¡è°ƒç”¨åè§¦å‘Nodeçš„`MaxListenersExceededWarning`ã€‚

      let lastResponseId: string = previousResponseId;

      // If there are unresolved function calls from a previously cancelled run
      // we have to emit dummy tool outputs so that the API no longer expects
      // them.  We prepend them to the userâ€‘supplied input so they appear
      // first in the conversation turn.
      // å¦‚æœæœ‰æ¥è‡ªå…ˆå‰å–æ¶ˆè¿è¡Œçš„æœªè§£å†³å‡½æ•°è°ƒç”¨ï¼Œæˆ‘ä»¬å¿…é¡»å‘å‡ºè™šæ‹Ÿå·¥å…·è¾“å‡ºï¼Œ
      // ä»¥ä¾¿APIä¸å†æœŸæœ›å®ƒä»¬ã€‚æˆ‘ä»¬å°†å®ƒä»¬é¢„å…ˆæ·»åŠ åˆ°ç”¨æˆ·æä¾›çš„è¾“å…¥ä¸­ï¼Œä»¥ä¾¿å®ƒä»¬
      // åœ¨å¯¹è¯è½®æ¬¡ä¸­é¦–å…ˆå‡ºç°ã€‚
      const abortOutputs: Array<ResponseInputItem> = [];
      if (this.pendingAborts.size > 0) {
        for (const id of this.pendingAborts) {
          abortOutputs.push({
            type: "function_call_output",
            call_id: id,
            output: JSON.stringify({
              output: "aborted",
              metadata: { exit_code: 1, duration_seconds: 0 },
            }),
          } as ResponseInputItem.FunctionCallOutput);
        }
        // Once converted the pending list can be cleared.
        // ä¸€æ—¦è½¬æ¢ï¼Œå¾…å¤„ç†åˆ—è¡¨å°±å¯ä»¥è¢«æ¸…é™¤ã€‚
        this.pendingAborts.clear();
      }

      let turnInput = [...abortOutputs, ...input];

      this.onLoading(true);

      const staged: Array<ResponseItem | undefined> = [];
      const stageItem = (item: ResponseItem) => {
        // Ignore any stray events that belong to older generations.
        // å¿½ç•¥å±äºè¾ƒæ—§ä¸–ä»£çš„ä»»ä½•æ¸¸ç¦»äº‹ä»¶ã€‚
        if (thisGeneration !== this.generation) {
          return;
        }

        // Store the item so the final flush can still operate on a complete list.
        // We'll nil out entries once they're delivered.
        // å­˜å‚¨é¡¹ç›®ï¼Œä»¥ä¾¿æœ€ç»ˆåˆ·æ–°ä»ç„¶å¯ä»¥å¯¹å®Œæ•´åˆ—è¡¨è¿›è¡Œæ“ä½œã€‚
        // ä¸€æ—¦å®ƒä»¬è¢«ä¼ é€’ï¼Œæˆ‘ä»¬å°†æ¸…ç©ºæ¡ç›®ã€‚
        const idx = staged.push(item) - 1;

        // Instead of emitting synchronously we schedule a shortâ€‘delay delivery.
        // This accomplishes two things:
        //   1. The UI still sees new messages almost immediately, creating the
        //      perception of realâ€‘time updates.
        //   2. If the user calls `cancel()` in the small window right after the
        //      item was staged we can still abort the delivery because the
        //      generation counter will have been bumped by `cancel()`.
        // æˆ‘ä»¬ä¸æ˜¯åŒæ­¥å‘å‡ºï¼Œè€Œæ˜¯è®¡åˆ’ä¸€ä¸ªçŸ­å»¶è¿Ÿçš„ä¼ é€’ã€‚
        // è¿™å®ç°äº†ä¸¤ä»¶äº‹ï¼š
        //   1. UIå‡ ä¹ç«‹å³çœ‹åˆ°æ–°æ¶ˆæ¯ï¼Œåˆ›é€ äº†å®æ—¶æ›´æ–°çš„æ„Ÿè§‰ã€‚
        //   2. å¦‚æœç”¨æˆ·åœ¨é¡¹ç›®è¢«æš‚å­˜åçš„å°çª—å£ä¸­è°ƒç”¨`cancel()`ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥
        //      ä¸­æ­¢ä¼ é€’ï¼Œå› ä¸ºä¸–ä»£è®¡æ•°å™¨å°†è¢«`cancel()`å¢åŠ ã€‚
        setTimeout(() => {
          if (
            thisGeneration === this.generation &&
            !this.canceled &&
            !this.hardAbort.signal.aborted
          ) {
            this.onItem(item);
            // Mark as delivered so flush won't re-emit it
            // æ ‡è®°ä¸ºå·²ä¼ é€’ï¼Œä»¥ä¾¿åˆ·æ–°ä¸ä¼šé‡æ–°å‘å‡ºå®ƒ
            staged[idx] = undefined;
          }
        }, 10);
      };

      while (turnInput.length > 0) {
        if (this.canceled || this.hardAbort.signal.aborted) {
          this.onLoading(false);
          return;
        }
        // send request to openAI
        // å‘openAIå‘é€è¯·æ±‚
        for (const item of turnInput) {
          stageItem(item as ResponseItem);
        }
        // Send request to OpenAI with retry on timeout
        // å‘OpenAIå‘é€è¯·æ±‚ï¼Œè¶…æ—¶æ—¶é‡è¯•
        let stream;

        // Retry loop for transient errors. Up to MAX_RETRIES attempts.
        // ä¸´æ—¶é”™è¯¯çš„é‡è¯•å¾ªç¯ã€‚æœ€å¤šå°è¯•MAX_RETRIESæ¬¡ã€‚
        const MAX_RETRIES = 5;
        for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
          try {
            let reasoning: Reasoning | undefined;
            if (this.model.startsWith("o")) {
              reasoning = { effort: "high" };
              if (this.model === "o3" || this.model === "o4-mini") {
                // @ts-expect-error waiting for API type update
                reasoning.summary = "auto";
              }
            }
            const mergedInstructions = [prefix, this.instructions]
              .filter(Boolean)
              .join("\n");
            if (isLoggingEnabled()) {
              log(
                `instructions (length ${mergedInstructions.length}): ${mergedInstructions}`,
              );
            }
            // eslint-disable-next-line no-await-in-loop
            stream = await this.oai.responses.create({
              model: this.model,
              instructions: mergedInstructions,
              previous_response_id: lastResponseId || undefined,
              input: turnInput,
              stream: true,
              parallel_tool_calls: false,
              reasoning,
              tools: [
                {
                  type: "function",
                  name: "shell",
                  description: "Runs a shell command, and returns its output.",
                  strict: false,
                  parameters: {
                    type: "object",
                    properties: {
                      command: { type: "array", items: { type: "string" } },
                      workdir: {
                        type: "string",
                        description: "The working directory for the command.",
                      },
                      timeout: {
                        type: "number",
                        description:
                          "The maximum time to wait for the command to complete in milliseconds.",
                      },
                    },
                    required: ["command"],
                    additionalProperties: false,
                  },
                },
              ],
            });
            break;
          } catch (error) {
            const isTimeout = error instanceof APIConnectionTimeoutError;
            // Lazily look up the APIConnectionError class at runtime to
            // accommodate the test environment's minimal OpenAI mocks which
            // do not define the class.  Falling back to `false` when the
            // export is absent ensures the check never throws.
            // åœ¨è¿è¡Œæ—¶å»¶è¿ŸæŸ¥æ‰¾APIConnectionErrorç±»ï¼Œä»¥é€‚åº”æµ‹è¯•ç¯å¢ƒä¸­
            // æœ€å°åŒ–çš„OpenAIæ¨¡æ‹Ÿï¼ˆå®ƒä»¬æ²¡æœ‰å®šä¹‰è¯¥ç±»ï¼‰ã€‚å½“å¯¼å‡ºä¸å­˜åœ¨æ—¶
            // å›é€€åˆ°`false`ç¡®ä¿æ£€æŸ¥æ°¸è¿œä¸ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            const ApiConnErrCtor = (OpenAI as any).APIConnectionError as  // eslint-disable-next-line @typescript-eslint/no-explicit-any
              | (new (...args: any) => Error)
              | undefined;
            const isConnectionError = ApiConnErrCtor
              ? error instanceof ApiConnErrCtor
              : false;
            // eslint-disable-next-line @typescript-eslint/no-explicit-any
            const errCtx = error as any;
            const status =
              errCtx?.status ?? errCtx?.httpStatus ?? errCtx?.statusCode;
            const isServerError = typeof status === "number" && status >= 500;
            if (
              (isTimeout || isServerError || isConnectionError) &&
              attempt < MAX_RETRIES
            ) {
              log(
                `OpenAI request failed (attempt ${attempt}/${MAX_RETRIES}), retrying...`,
              );
              continue;
            }

            const isTooManyTokensError =
              (errCtx.param === "max_tokens" ||
                (typeof errCtx.message === "string" &&
                  /max_tokens is too large/i.test(errCtx.message))) &&
              errCtx.type === "invalid_request_error";

            if (isTooManyTokensError) {
              this.onItem({
                id: `error-${Date.now()}`,
                type: "message",
                role: "system",
                content: [
                  {
                    type: "input_text",
                    text: "âš ï¸  The current request exceeds the maximum context length supported by the chosen model. Please shorten the conversation, run /clear, or switch to a model with a larger context window and try again.",
                  },
                ],
              });
              this.onLoading(false);
              return;
            }

            const isRateLimit =
              status === 429 ||
              errCtx.code === "rate_limit_exceeded" ||
              errCtx.type === "rate_limit_exceeded" ||
              /rate limit/i.test(errCtx.message ?? "");
            if (isRateLimit) {
              if (attempt < MAX_RETRIES) {
                // Exponential backoff: base wait * 2^(attempt-1), or use suggested retry time
                // if provided.
                // æŒ‡æ•°é€€é¿ï¼šåŸºæœ¬ç­‰å¾…æ—¶é—´ * 2^(å°è¯•æ¬¡æ•°-1)ï¼Œæˆ–è€…ä½¿ç”¨å»ºè®®çš„é‡è¯•æ—¶é—´ï¼ˆå¦‚æœæä¾›ï¼‰ã€‚
                let delayMs = RATE_LIMIT_RETRY_WAIT_MS * 2 ** (attempt - 1);

                // Parse suggested retry time from error message, e.g., "Please try again in 1.3s"
                // ä»é”™è¯¯æ¶ˆæ¯ä¸­è§£æå»ºè®®çš„é‡è¯•æ—¶é—´ï¼Œä¾‹å¦‚ï¼Œ"Please try again in 1.3s"
                const msg = errCtx?.message ?? "";
                const m = /(?:retry|try) again in ([\d.]+)s/i.exec(msg);
                if (m && m[1]) {
                  const suggested = parseFloat(m[1]) * 1000;
                  if (!Number.isNaN(suggested)) {
                    delayMs = suggested;
                  }
                }
                log(
                  `OpenAI rate limit exceeded (attempt ${attempt}/${MAX_RETRIES}), retrying in ${Math.round(
                    delayMs,
                  )} ms...`,
                );
                // eslint-disable-next-line no-await-in-loop
                await new Promise((resolve) => setTimeout(resolve, delayMs));
                continue;
              } else {
                // We have exhausted all retry attempts. Surface a message so the user understands
                // why the request failed and can decide how to proceed (e.g. wait and retry later
                // or switch to a different model / account).
                // æˆ‘ä»¬å·²ç”¨å°½æ‰€æœ‰é‡è¯•å°è¯•ã€‚æ˜¾ç¤ºä¸€æ¡æ¶ˆæ¯ï¼Œè®©ç”¨æˆ·äº†è§£è¯·æ±‚å¤±è´¥çš„åŸå› ï¼Œ
                // å¹¶å¯ä»¥å†³å®šå¦‚ä½•ç»§ç»­ï¼ˆä¾‹å¦‚ï¼Œç­‰å¾…å¹¶ç¨åé‡è¯•ï¼Œæˆ–åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹/è´¦æˆ·ï¼‰ã€‚

                const errorDetails = [
                  `Status: ${status || "unknown"}`,
                  `Code: ${errCtx.code || "unknown"}`,
                  `Type: ${errCtx.type || "unknown"}`,
                  `Message: ${errCtx.message || "unknown"}`,
                ].join(", ");

                this.onItem({
                  id: `error-${Date.now()}`,
                  type: "message",
                  role: "system",
                  content: [
                    {
                      type: "input_text",
                      text: `âš ï¸  Rate limit reached. Error details: ${errorDetails}. Please try again later.`,
                    },
                  ],
                });

                this.onLoading(false);
                return;
              }
            }

            const isClientError =
              (typeof status === "number" &&
                status >= 400 &&
                status < 500 &&
                status !== 429) ||
              errCtx.code === "invalid_request_error" ||
              errCtx.type === "invalid_request_error";
            if (isClientError) {
              this.onItem({
                id: `error-${Date.now()}`,
                type: "message",
                role: "system",
                content: [
                  {
                    type: "input_text",
                    // Surface the request ID when it is present on the error so users
                    // can reference it when contacting support or inspecting logs.
                    // å½“é”™è¯¯ä¸­å­˜åœ¨è¯·æ±‚IDæ—¶æ˜¾ç¤ºå®ƒï¼Œä»¥ä¾¿ç”¨æˆ·åœ¨è”ç³»æ”¯æŒæˆ–æ£€æŸ¥æ—¥å¿—æ—¶å¯ä»¥å¼•ç”¨å®ƒã€‚
                    text: (() => {
                      const reqId =
                        (
                          errCtx as Partial<{
                            request_id?: string;
                            requestId?: string;
                          }>
                        )?.request_id ??
                        (
                          errCtx as Partial<{
                            request_id?: string;
                            requestId?: string;
                          }>
                        )?.requestId;

                      const errorDetails = [
                        `Status: ${status || "unknown"}`,
                        `Code: ${errCtx.code || "unknown"}`,
                        `Type: ${errCtx.type || "unknown"}`,
                        `Message: ${errCtx.message || "unknown"}`,
                      ].join(", ");

                      return `âš ï¸  OpenAI rejected the request${
                        reqId ? ` (request ID: ${reqId})` : ""
                      }. Error details: ${errorDetails}. Please verify your settings and try again.`;
                    })(),
                  },
                ],
              });
              this.onLoading(false);
              return;
            }
            throw error;
          }
        }
        turnInput = []; // clear turn input, prepare for function call results
                        // æ¸…é™¤è½®æ¬¡è¾“å…¥ï¼Œå‡†å¤‡å‡½æ•°è°ƒç”¨ç»“æœ

        // If the user requested cancellation while we were awaiting the network
        // request, abort immediately before we start handling the stream.
        // å¦‚æœç”¨æˆ·åœ¨æˆ‘ä»¬ç­‰å¾…ç½‘ç»œè¯·æ±‚æ—¶è¯·æ±‚å–æ¶ˆï¼Œåœ¨æˆ‘ä»¬å¼€å§‹å¤„ç†æµä¹‹å‰ç«‹å³ä¸­æ­¢ã€‚
        if (this.canceled || this.hardAbort.signal.aborted) {
          // `stream` is defined; abort to avoid wasting tokens/server work
          // `stream`å·²å®šä¹‰ï¼›ä¸­æ­¢ä»¥é¿å…æµªè´¹ä»¤ç‰Œ/æœåŠ¡å™¨å·¥ä½œ
          try {
            (
              stream as { controller?: { abort?: () => void } }
            )?.controller?.abort?.();
          } catch {
            /* ignore */
          }
          this.onLoading(false);
          return;
        }

        // Keep track of the active stream so it can be aborted on demand.
        // è·Ÿè¸ªæ´»åŠ¨æµï¼Œä»¥ä¾¿å¯ä»¥æŒ‰éœ€ä¸­æ­¢ã€‚
        this.currentStream = stream;

        // guard against an undefined stream before iterating
        // åœ¨è¿­ä»£ä¹‹å‰é˜²æ­¢æœªå®šä¹‰çš„æµ
        if (!stream) {
          this.onLoading(false);
          log("AgentLoop.run(): stream is undefined");
          return;
        }

        try {
          // eslint-disable-next-line no-await-in-loop
          for await (const event of stream) {
            if (isLoggingEnabled()) {
              log(`AgentLoop.run(): response event ${event.type}`);
            }

            // process and surface each item (noâ€‘op until we can depend on streaming events)
            // å¤„ç†å¹¶æ˜¾ç¤ºæ¯ä¸ªé¡¹ç›®ï¼ˆåœ¨æˆ‘ä»¬å¯ä»¥ä¾èµ–æµå¼äº‹ä»¶ä¹‹å‰æ˜¯æ— æ“ä½œçš„ï¼‰
            if (event.type === "response.output_item.done") {
              const item = event.item;
              // 1) if it's a reasoning item, annotate it
              // 1) å¦‚æœæ˜¯æ¨ç†é¡¹ï¼Œå¯¹å…¶è¿›è¡Œæ³¨é‡Š
              type ReasoningItem = { type?: string; duration_ms?: number };
              const maybeReasoning = item as ReasoningItem;
              if (maybeReasoning.type === "reasoning") {
                maybeReasoning.duration_ms = Date.now() - thinkingStart;
              }
              if (item.type === "function_call") {
                // Track outstanding tool call so we can abort later if needed.
                // The item comes from the streaming response, therefore it has
                // either `id` (chat) or `call_id` (responses) â€“ we normalise
                // by reading both.
                // è·Ÿè¸ªæœªå®Œæˆçš„å·¥å…·è°ƒç”¨ï¼Œä»¥ä¾¿åœ¨éœ€è¦æ—¶å¯ä»¥ç¨åä¸­æ­¢ã€‚
                // è¯¥é¡¹æ¥è‡ªæµå¼å“åº”ï¼Œå› æ­¤å®ƒæœ‰`id`ï¼ˆèŠå¤©ï¼‰æˆ–`call_id`ï¼ˆå“åº”ï¼‰
                // - æˆ‘ä»¬é€šè¿‡è¯»å–ä¸¤è€…æ¥æ ‡å‡†åŒ–ã€‚
                const callId =
                  (item as { call_id?: string; id?: string }).call_id ??
                  (item as { id?: string }).id;
                if (callId) {
                  this.pendingAborts.add(callId);
                }
              } else {
                stageItem(item as ResponseItem);
              }
            }

            if (event.type === "response.completed") {
              if (thisGeneration === this.generation && !this.canceled) {
                for (const item of event.response.output) {
                  stageItem(item as ResponseItem);
                }
              }
              if (event.response.status === "completed") {
                // TODO: remove this once we can depend on streaming events
                // TODO: ä¸€æ—¦æˆ‘ä»¬å¯ä»¥ä¾èµ–æµå¼äº‹ä»¶å°±åˆ é™¤è¿™ä¸ª
                const newTurnInput = await this.processEventsWithoutStreaming(
                  event.response.output,
                  stageItem,
                );
                turnInput = newTurnInput;
              }
              lastResponseId = event.response.id;
              this.onLastResponseId(event.response.id);
            }
          }
        } catch (err: unknown) {
          // Gracefully handle an abort triggered via `cancel()` so that the
          // consumer does not see an unhandled exception.
          // ä¼˜é›…åœ°å¤„ç†é€šè¿‡`cancel()`è§¦å‘çš„ä¸­æ­¢ï¼Œä»¥ä¾¿æ¶ˆè´¹è€…ä¸ä¼šçœ‹åˆ°æœªå¤„ç†çš„å¼‚å¸¸ã€‚
          if (err instanceof Error && err.name === "AbortError") {
            if (!this.canceled) {
              // It was aborted for some other reason; surface the error.
              // å®ƒå› å…¶ä»–åŸå› è¢«ä¸­æ­¢ï¼›æ˜¾ç¤ºé”™è¯¯ã€‚
              throw err;
            }
            this.onLoading(false);
            return;
          }
          throw err;
        } finally {
          this.currentStream = null;
        }

        log(
          `Turn inputs (${turnInput.length}) - ${turnInput
            .map((i) => i.type)
            .join(", ")}`,
        );
      }

      // Flush staged items if the run concluded successfully (i.e. the user did
      // not invoke cancel() or terminate() during the turn).
      // å¦‚æœè¿è¡ŒæˆåŠŸç»“æŸï¼ˆå³ç”¨æˆ·åœ¨è½®æ¬¡æœŸé—´æ²¡æœ‰è°ƒç”¨cancel()æˆ–terminate()ï¼‰ï¼Œ
      // åˆ™åˆ·æ–°æš‚å­˜é¡¹ã€‚
      const flush = () => {
        if (
          !this.canceled &&
          !this.hardAbort.signal.aborted &&
          thisGeneration === this.generation
        ) {
          // Only emit items that weren't already delivered above
          // åªå‘å‡ºä¸Šé¢å°šæœªä¼ é€’çš„é¡¹ç›®
          for (const item of staged) {
            if (item) {
              this.onItem(item);
            }
          }
        }

        // At this point the turn finished without the user invoking
        // `cancel()`.  Any outstanding functionâ€‘calls must therefore have been
        // satisfied, so we can safely clear the set that tracks pending aborts
        // to avoid emitting duplicate synthetic outputs in subsequent runs.
        // åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œè½®æ¬¡åœ¨ç”¨æˆ·æ²¡æœ‰è°ƒç”¨`cancel()`çš„æƒ…å†µä¸‹ç»“æŸã€‚
        // å› æ­¤ï¼Œä»»ä½•æœªå®Œæˆçš„å‡½æ•°è°ƒç”¨éƒ½å¿…é¡»å·²ç»å¾—åˆ°æ»¡è¶³ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å®‰å…¨åœ°
        // æ¸…é™¤è·Ÿè¸ªå¾…å¤„ç†ä¸­æ­¢çš„é›†åˆï¼Œä»¥é¿å…åœ¨åç»­è¿è¡Œä¸­å‘å‡ºé‡å¤çš„åˆæˆè¾“å‡ºã€‚
        this.pendingAborts.clear();
        // Now emit system messages recording the perâ€‘turn *and* cumulative
        // thinking times so UIs and tests can surface/verify them.
        // ç°åœ¨å‘å‡ºç³»ç»Ÿæ¶ˆæ¯ï¼Œè®°å½•æ¯è½®*å’Œ*ç´¯è®¡æ€è€ƒæ—¶é—´ï¼Œä»¥ä¾¿UIå’Œæµ‹è¯•å¯ä»¥æ˜¾ç¤º/éªŒè¯å®ƒä»¬ã€‚
        // const thinkingEnd = Date.now();

        // 1) Perâ€‘turn measurement â€“ exact time spent between request and
        //    response for *this* command.
        // 1) æ¯è½®æµ‹é‡ - æ­¤å‘½ä»¤çš„è¯·æ±‚å’Œå“åº”ä¹‹é—´èŠ±è´¹çš„ç¡®åˆ‡æ—¶é—´ã€‚
        // this.onItem({
        //   id: `thinking-${thinkingEnd}`,
        //   type: "message",
        //   role: "system",
        //   content: [
        //     {
        //       type: "input_text",
        //       text: `ğŸ¤”  Thinking time: ${Math.round(
        //         (thinkingEnd - thinkingStart) / 1000
        //       )} s`,
        //     },
        //   ],
        // });

        // 2) Sessionâ€‘wide cumulative counter so users can track overall wait
        //    time across multiple turns.
        // 2) ä¼šè¯èŒƒå›´çš„ç´¯è®¡è®¡æ•°å™¨ï¼Œä»¥ä¾¿ç”¨æˆ·å¯ä»¥è·Ÿè¸ªå¤šä¸ªè½®æ¬¡çš„æ€»ç­‰å¾…æ—¶é—´ã€‚
        // this.cumulativeThinkingMs += thinkingEnd - thinkingStart;
        // this.onItem({
        //   id: `thinking-total-${thinkingEnd}`,
        //   type: "message",
        //   role: "system",
        //   content: [
        //     {
        //       type: "input_text",
        //       text: `â±  Total thinking time: ${Math.round(
        //         this.cumulativeThinkingMs / 1000
        //       )} s`,
        //     },
        //   ],
        // });

        this.onLoading(false);
      };

      // Delay flush slightly to allow a nearâ€‘simultaneous cancel() to land.
      // ç¨å¾®å»¶è¿Ÿåˆ·æ–°ï¼Œä»¥å…è®¸å‡ ä¹åŒæ—¶çš„cancel()è½åœ°ã€‚
      setTimeout(flush, 30);
      // End of main logic. The corresponding catch block for the wrapper at the
      // start of this method follows next.
      // ä¸»é€»è¾‘ç»“æŸã€‚æ­¤æ–¹æ³•å¼€å§‹å¤„çš„åŒ…è£…å™¨çš„ç›¸åº”catchå—æ¥ä¸‹æ¥ã€‚
    } catch (err) {
      // Handle known transient network/streaming issues so they do not crash the
      // CLI. We currently match Node/undici's `ERR_STREAM_PREMATURE_CLOSE`
      // error which manifests when the HTTP/2 stream terminates unexpectedly
      // (e.g. during brief network hiccups).
      // å¤„ç†å·²çŸ¥çš„ä¸´æ—¶ç½‘ç»œ/æµé—®é¢˜ï¼Œä»¥ä¾¿å®ƒä»¬ä¸ä¼šä½¿CLIå´©æºƒã€‚
      // æˆ‘ä»¬ç›®å‰åŒ¹é…Node/undiciçš„`ERR_STREAM_PREMATURE_CLOSE`é”™è¯¯ï¼Œ
      // è¯¥é”™è¯¯åœ¨HTTP/2æµæ„å¤–ç»ˆæ­¢æ—¶è¡¨ç°å‡ºæ¥ï¼ˆä¾‹å¦‚ï¼Œåœ¨çŸ­æš‚çš„ç½‘ç»œä¸­æ–­æœŸé—´ï¼‰ã€‚

      const isPrematureClose =
        err instanceof Error &&
        // eslint-disable-next-line
        ((err as any).code === "ERR_STREAM_PREMATURE_CLOSE" ||
          err.message?.includes("Premature close"));

      if (isPrematureClose) {
        try {
          this.onItem({
            id: `error-${Date.now()}`,
            type: "message",
            role: "system",
            content: [
              {
                type: "input_text",
                text: "âš ï¸  Connection closed prematurely while waiting for the model. Please try again.",
              },
            ],
          });
        } catch {
          /* noâ€‘op â€“ emitting the error message is bestâ€‘effort */
          /* æ— æ“ä½œ - å‘å‡ºé”™è¯¯æ¶ˆæ¯æ˜¯å°½åŠ›è€Œä¸º */
        }
        this.onLoading(false);
        return;
      }

      // -------------------------------------------------------------------
      // Catchâ€‘all handling for other network or serverâ€‘side issues so that
      // transient failures do not crash the CLI. We intentionally keep the
      // detection logic conservative to avoid masking programming errors. A
      // failure is treated as retryâ€‘worthy/userâ€‘visible when any of the
      // following apply:
      //   â€¢ the error carries a recognised Node.js network errno â€‘ style code
      //     (e.g. ECONNRESET, ETIMEDOUT â€¦)
      //   â€¢ the OpenAI SDK attached an HTTP `status` >= 500 indicating a
      //     serverâ€‘side problem.
      //   â€¢ the error is model specific and detected in stream.
      // If matched we emit a single system message to inform the user and
      // resolve gracefully so callers can choose to retry.
      // -------------------------------------------------------------------
      // -------------------------------------------------------------------
      // å¯¹å…¶ä»–ç½‘ç»œæˆ–æœåŠ¡å™¨ç«¯é—®é¢˜çš„å…¨é¢å¤„ç†ï¼Œä»¥ä¾¿ä¸´æ—¶æ•…éšœä¸ä¼šä½¿CLIå´©æºƒã€‚
      // æˆ‘ä»¬æœ‰æ„ä¿æŒæ£€æµ‹é€»è¾‘ä¿å®ˆï¼Œä»¥é¿å…æ©ç›–ç¼–ç¨‹é”™è¯¯ã€‚
      // å½“ä»¥ä¸‹ä»»ä½•ä¸€é¡¹é€‚ç”¨æ—¶ï¼Œæ•…éšœè¢«è§†ä¸ºå€¼å¾—é‡è¯•/ç”¨æˆ·å¯è§ï¼š
      //   â€¢ é”™è¯¯å¸¦æœ‰å…¬è®¤çš„Node.jsç½‘ç»œerrnoé£æ ¼ä»£ç ï¼ˆä¾‹å¦‚ECONNRESETï¼ŒETIMEDOUTâ€¦ï¼‰
      //   â€¢ OpenAI SDKé™„åŠ äº†HTTP `status` >= 500ï¼Œè¡¨ç¤ºæœåŠ¡å™¨ç«¯é—®é¢˜ã€‚
      //   â€¢ é”™è¯¯æ˜¯ç‰¹å®šäºæ¨¡å‹çš„ï¼Œå¹¶åœ¨æµä¸­æ£€æµ‹åˆ°ã€‚
      // å¦‚æœåŒ¹é…ï¼Œæˆ‘ä»¬å‘å‡ºä¸€ä¸ªç³»ç»Ÿæ¶ˆæ¯é€šçŸ¥ç”¨æˆ·ï¼Œå¹¶ä¼˜é›…åœ°è§£å†³ï¼Œä»¥ä¾¿è°ƒç”¨è€…å¯ä»¥é€‰æ‹©é‡è¯•ã€‚
      // -------------------------------------------------------------------

      const NETWORK_ERRNOS = new Set([
        "ECONNRESET",
        "ECONNREFUSED",
        "EPIPE",
        "ENOTFOUND",
        "ETIMEDOUT",
        "EAI_AGAIN",
      ]);

      const isNetworkOrServerError = (() => {
        if (!err || typeof err !== "object") {
          return false;
        }
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const e: any = err;

        // Direct instance check for connection errors thrown by the OpenAI SDK.
        // å¯¹OpenAI SDKæŠ›å‡ºçš„è¿æ¥é”™è¯¯è¿›è¡Œç›´æ¥å®ä¾‹æ£€æŸ¥ã€‚
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const ApiConnErrCtor = (OpenAI as any).APIConnectionError as  // eslint-disable-next-line @typescript-eslint/no-explicit-any
          | (new (...args: any) => Error)
          | undefined;
        if (ApiConnErrCtor && e instanceof ApiConnErrCtor) {
          return true;
        }

        if (typeof e.code === "string" && NETWORK_ERRNOS.has(e.code)) {
          return true;
        }

        // When the OpenAI SDK nests the underlying network failure inside the
        // `cause` property we surface it as well so callers do not see an
        // unhandled exception for errors like ENOTFOUND, ECONNRESET â€¦
        // å½“OpenAI SDKå°†åº•å±‚ç½‘ç»œæ•…éšœåµŒå¥—åœ¨`cause`å±æ€§ä¸­æ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå°†å…¶æš´éœ²å‡ºæ¥ï¼Œ
        // ä»¥ä¾¿è°ƒç”¨è€…ä¸ä¼šçœ‹åˆ°è¯¸å¦‚ENOTFOUNDã€ECONNRESETç­‰é”™è¯¯çš„æœªå¤„ç†å¼‚å¸¸ã€‚
        if (
          e.cause &&
          typeof e.cause === "object" &&
          NETWORK_ERRNOS.has((e.cause as { code?: string }).code ?? "")
        ) {
          return true;
        }

        if (typeof e.status === "number" && e.status >= 500) {
          return true;
        }

        // Fallback to a heuristic string match so we still catch future SDK
        // variations without enumerating every errno.
        // å›é€€åˆ°å¯å‘å¼å­—ç¬¦ä¸²åŒ¹é…ï¼Œä»¥ä¾¿æˆ‘ä»¬ä»ç„¶èƒ½æ•è·æœªæ¥çš„SDKå˜ä½“ï¼Œ
        // è€Œæ— éœ€æšä¸¾æ¯ä¸ªerrnoã€‚
        if (
          typeof e.message === "string" &&
          /network|socket|stream/i.test(e.message)
        ) {
          return true;
        }

        return false;
      })();

      if (isNetworkOrServerError) {
        try {
          const msgText =
            "âš ï¸  Network error while contacting OpenAI. Please check your connection and try again.";
          this.onItem({
            id: `error-${Date.now()}`,
            type: "message",
            role: "system",
            content: [
              {
                type: "input_text",
                text: msgText,
              },
            ],
          });
        } catch {
          /* bestâ€‘effort */
          /* å°½åŠ›è€Œä¸º */
        }
        this.onLoading(false);
        return;
      }

      const isInvalidRequestError = () => {
        if (!err || typeof err !== "object") {
          return false;
        }
        // eslint-disable-next-line @typescript-eslint/no-explicit-any
        const e: any = err;

        if (
          e.type === "invalid_request_error" &&
          e.code === "model_not_found"
        ) {
          return true;
        }

        if (
          e.cause &&
          e.cause.type === "invalid_request_error" &&
          e.cause.code === "model_not_found"
        ) {
          return true;
        }

        return false;
      };

      if (isInvalidRequestError()) {
        try {
          // Extract request ID and error details from the error object
          // ä»é”™è¯¯å¯¹è±¡ä¸­æå–è¯·æ±‚IDå’Œé”™è¯¯è¯¦æƒ…

          // eslint-disable-next-line @typescript-eslint/no-explicit-any
          const e: any = err;

          const reqId =
            e.request_id ??
            (e.cause && e.cause.request_id) ??
            (e.cause && e.cause.requestId);

          const errorDetails = [
            `Status: ${e.status || (e.cause && e.cause.status) || "unknown"}`,
            `Code: ${e.code || (e.cause && e.cause.code) || "unknown"}`,
            `Type: ${e.type || (e.cause && e.cause.type) || "unknown"}`,
            `Message: ${
              e.message || (e.cause && e.cause.message) || "unknown"
            }`,
          ].join(", ");

          const msgText = `âš ï¸  OpenAI rejected the request${
            reqId ? ` (request ID: ${reqId})` : ""
          }. Error details: ${errorDetails}. Please verify your settings and try again.`;

          this.onItem({
            id: `error-${Date.now()}`,
            type: "message",
            role: "system",
            content: [
              {
                type: "input_text",
                text: msgText,
              },
            ],
          });
        } catch {
          /* best-effort */
          /* å°½åŠ›è€Œä¸º */
        }
        this.onLoading(false);
        return;
      }

      // Reâ€‘throw all other errors so upstream handlers can decide what to do.
      // é‡æ–°æŠ›å‡ºæ‰€æœ‰å…¶ä»–é”™è¯¯ï¼Œä»¥ä¾¿ä¸Šæ¸¸å¤„ç†ç¨‹åºå¯ä»¥å†³å®šå¦‚ä½•å¤„ç†ã€‚
      throw err;
    }
  }

  // we need until we can depend on streaming events
  // æˆ‘ä»¬éœ€è¦ç›´åˆ°æˆ‘ä»¬å¯ä»¥ä¾èµ–æµå¼äº‹ä»¶
  private async processEventsWithoutStreaming(
    output: Array<ResponseInputItem>,
    emitItem: (item: ResponseItem) => void,
  ): Promise<Array<ResponseInputItem>> {
    // If the agent has been canceled we should shortâ€‘circuit immediately to
    // avoid any further processing (including potentially expensive tool
    // calls). Returning an empty array ensures the main runâ€‘loop terminates
    // promptly.
    // å¦‚æœä»£ç†å·²è¢«å–æ¶ˆï¼Œæˆ‘ä»¬åº”è¯¥ç«‹å³çŸ­è·¯ï¼Œä»¥é¿å…ä»»ä½•è¿›ä¸€æ­¥çš„å¤„ç†ï¼ˆåŒ…æ‹¬å¯èƒ½æ˜‚è´µçš„å·¥å…·è°ƒç”¨ï¼‰ã€‚
    // è¿”å›ç©ºæ•°ç»„ç¡®ä¿ä¸»è¿è¡Œå¾ªç¯åŠæ—¶ç»ˆæ­¢ã€‚
    if (this.canceled) {
      return [];
    }
    const turnInput: Array<ResponseInputItem> = [];
    for (const item of output) {
      if (item.type === "function_call") {
        if (alreadyProcessedResponses.has(item.id)) {
          continue;
        }
        alreadyProcessedResponses.add(item.id);
        // eslint-disable-next-line no-await-in-loop
        const result = await this.handleFunctionCall(item);
        turnInput.push(...result);
      }
      emitItem(item as ResponseItem);
    }
    return turnInput;
  }
}

const prefix = `You are operating as and within the Codex CLI, a terminal-based agentic coding assistant built by OpenAI. It wraps OpenAI models to enable natural language interaction with a local codebase. You are expected to be precise, safe, and helpful.
// ä½ ä½œä¸ºå¹¶åœ¨Codex CLIå†…è¿è¡Œï¼Œè¿™æ˜¯ä¸€ä¸ªç”±OpenAIæ„å»ºçš„åŸºäºç»ˆç«¯çš„æ™ºèƒ½ç¼–ç åŠ©æ‰‹ã€‚å®ƒå°è£…äº†OpenAIæ¨¡å‹ï¼Œä»¥å®ç°ä¸æœ¬åœ°ä»£ç åº“çš„è‡ªç„¶è¯­è¨€äº¤äº’ã€‚ä½ åº”å½“ç²¾ç¡®ã€å®‰å…¨ä¸”æœ‰å¸®åŠ©ã€‚

You can:
// ä½ å¯ä»¥ï¼š
- Receive user prompts, project context, and files.
// - æ¥æ”¶ç”¨æˆ·æç¤ºã€é¡¹ç›®ä¸Šä¸‹æ–‡å’Œæ–‡ä»¶ã€‚
- Stream responses and emit function calls (e.g., shell commands, code edits).
// - æµå¼ä¼ è¾“å“åº”å¹¶å‘å‡ºå‡½æ•°è°ƒç”¨ï¼ˆä¾‹å¦‚ï¼Œshellå‘½ä»¤ã€ä»£ç ç¼–è¾‘ï¼‰ã€‚
- Apply patches, run commands, and manage user approvals based on policy.
// - åº”ç”¨è¡¥ä¸ã€è¿è¡Œå‘½ä»¤ï¼Œå¹¶æ ¹æ®ç­–ç•¥ç®¡ç†ç”¨æˆ·æ‰¹å‡†ã€‚
- Work inside a sandboxed, git-backed workspace with rollback support.
// - åœ¨å…·æœ‰å›æ»šæ”¯æŒçš„æ²™ç›’ã€gitæ”¯æŒçš„å·¥ä½œç©ºé—´å†…å·¥ä½œã€‚
- Log telemetry so sessions can be replayed or inspected later.
// - è®°å½•é¥æµ‹æ•°æ®ï¼Œä»¥ä¾¿ä¼šè¯å¯ä»¥ç¨åé‡æ”¾æˆ–æ£€æŸ¥ã€‚
- More details on your functionality are available at \`codex --help\`
// - æœ‰å…³ä½ åŠŸèƒ½çš„æ›´å¤šè¯¦æƒ…å¯åœ¨\`codex --help\`ä¸­è·å–

The Codex CLI is open-sourced. Don't confuse yourself with the old Codex language model built by OpenAI many moons ago (this is understandably top of mind for you!). Within this context, Codex refers to the open-source agentic coding interface.
// Codex CLIæ˜¯å¼€æºçš„ã€‚ä¸è¦å°†è‡ªå·±ä¸OpenAIå¾ˆä¹…ä»¥å‰æ„å»ºçš„æ—§Codexè¯­è¨€æ¨¡å‹æ··æ·†ï¼ˆè¿™å¯¹ä½ æ¥è¯´å¯ä»¥ç†è§£ä¸ºé¦–è¦è€ƒè™‘çš„äº‹æƒ…ï¼ï¼‰ã€‚åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­ï¼ŒCodexæŒ‡çš„æ˜¯å¼€æºçš„æ™ºèƒ½ç¼–ç æ¥å£ã€‚

You are an agent - please keep going until the user's query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved. If you are not sure about file content or codebase structure pertaining to the user's request, use your tools to read files and gather the relevant information: do NOT guess or make up an answer.
// ä½ æ˜¯ä¸€ä¸ªä»£ç† - è¯·ç»§ç»­ç›´åˆ°ç”¨æˆ·çš„æŸ¥è¯¢å®Œå…¨è§£å†³ï¼Œç„¶åå†ç»“æŸä½ çš„å›åˆå¹¶è®©ä½ç»™ç”¨æˆ·ã€‚åªæœ‰å½“ä½ ç¡®å®šé—®é¢˜å·²è§£å†³æ—¶æ‰ç»ˆæ­¢ä½ çš„å›åˆã€‚å¦‚æœä½ ä¸ç¡®å®šä¸ç”¨æˆ·è¯·æ±‚ç›¸å…³çš„æ–‡ä»¶å†…å®¹æˆ–ä»£ç åº“ç»“æ„ï¼Œè¯·ä½¿ç”¨ä½ çš„å·¥å…·é˜…è¯»æ–‡ä»¶å¹¶æ”¶é›†ç›¸å…³ä¿¡æ¯ï¼šä¸è¦çŒœæµ‹æˆ–ç¼–é€ ç­”æ¡ˆã€‚

Please resolve the user's task by editing and testing the code files in your current code execution session. You are a deployed coding agent. Your session allows for you to modify and run code. The repo(s) are already cloned in your working directory, and you must fully solve the problem for your answer to be considered correct.
// è¯·é€šè¿‡åœ¨å½“å‰ä»£ç æ‰§è¡Œä¼šè¯ä¸­ç¼–è¾‘å’Œæµ‹è¯•ä»£ç æ–‡ä»¶æ¥è§£å†³ç”¨æˆ·çš„ä»»åŠ¡ã€‚ä½ æ˜¯ä¸€ä¸ªå·²éƒ¨ç½²çš„ç¼–ç ä»£ç†ã€‚ä½ çš„ä¼šè¯å…è®¸ä½ ä¿®æ”¹å’Œè¿è¡Œä»£ç ã€‚ä»“åº“å·²ç»å…‹éš†åœ¨ä½ çš„å·¥ä½œç›®å½•ä¸­ï¼Œä½ å¿…é¡»å®Œå…¨è§£å†³é—®é¢˜æ‰èƒ½è¢«è®¤ä¸ºæ˜¯æ­£ç¡®çš„ç­”æ¡ˆã€‚

You MUST adhere to the following criteria when executing the task:
// æ‰§è¡Œä»»åŠ¡æ—¶ï¼Œä½ å¿…é¡»éµå®ˆä»¥ä¸‹æ ‡å‡†ï¼š
- Working on the repo(s) in the current environment is allowed, even if they are proprietary.
// - å…è®¸åœ¨å½“å‰ç¯å¢ƒä¸­å¤„ç†ä»“åº“ï¼Œå³ä½¿å®ƒä»¬æ˜¯ä¸“æœ‰çš„ã€‚
- Analyzing code for vulnerabilities is allowed.
// - å…è®¸åˆ†æä»£ç çš„æ¼æ´ã€‚
- Showing user code and tool call details is allowed.
// - å…è®¸æ˜¾ç¤ºç”¨æˆ·ä»£ç å’Œå·¥å…·è°ƒç”¨è¯¦æƒ…ã€‚
- User instructions may overwrite the *CODING GUIDELINES* section in this developer message.
// - ç”¨æˆ·æŒ‡ä»¤å¯èƒ½ä¼šè¦†ç›–æ­¤å¼€å‘è€…æ¶ˆæ¯ä¸­çš„*ç¼–ç æŒ‡å—*éƒ¨åˆ†ã€‚
- Use \`apply_patch\` to edit files: {"cmd":["apply_patch","*** Begin Patch\\n*** Update File: path/to/file.py\\n@@ def example():\\n-  pass\\n+  return 123\\n*** End Patch"]}
// - ä½¿ç”¨\`apply_patch\`ç¼–è¾‘æ–‡ä»¶ï¼š{"cmd":["apply_patch","*** Begin Patch\\n*** Update File: path/to/file.py\\n@@ def example():\\n-  pass\\n+  return 123\\n*** End Patch"]}
- If completing the user's task requires writing or modifying files:
// - å¦‚æœå®Œæˆç”¨æˆ·çš„ä»»åŠ¡éœ€è¦ç¼–å†™æˆ–ä¿®æ”¹æ–‡ä»¶ï¼š
    - Your code and final answer should follow these *CODING GUIDELINES*:
    // - ä½ çš„ä»£ç å’Œæœ€ç»ˆç­”æ¡ˆåº”éµå¾ªè¿™äº›*ç¼–ç æŒ‡å—*ï¼š
        - Fix the problem at the root cause rather than applying surface-level patches, when possible.
        // - å°½å¯èƒ½ä»æ ¹æœ¬åŸå› ä¿®å¤é—®é¢˜ï¼Œè€Œä¸æ˜¯åº”ç”¨è¡¨é¢çº§åˆ«çš„è¡¥ä¸ã€‚
        - Avoid unneeded complexity in your solution.
        // - é¿å…åœ¨ä½ çš„è§£å†³æ–¹æ¡ˆä¸­å‡ºç°ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚
            - Ignore unrelated bugs or broken tests; it is not your responsibility to fix them.
            // - å¿½ç•¥ä¸ç›¸å…³çš„é”™è¯¯æˆ–æŸåçš„æµ‹è¯•ï¼›ä¿®å¤å®ƒä»¬ä¸æ˜¯ä½ çš„è´£ä»»ã€‚
        - Update documentation as necessary.
        // - æ ¹æ®éœ€è¦æ›´æ–°æ–‡æ¡£ã€‚
        - Keep changes consistent with the style of the existing codebase. Changes should be minimal and focused on the task.
        // - ä¿æŒæ›´æ”¹ä¸ç°æœ‰ä»£ç åº“é£æ ¼ä¸€è‡´ã€‚æ›´æ”¹åº”å½“æ˜¯æœ€å°çš„ï¼Œå¹¶ä¸“æ³¨äºä»»åŠ¡ã€‚
            - Use \`git log\` and \`git blame\` to search the history of the codebase if additional context is required; internet access is disabled.
            // - å¦‚æœéœ€è¦é¢å¤–ä¸Šä¸‹æ–‡ï¼Œä½¿ç”¨\`git log\`å’Œ\`git blame\`æœç´¢ä»£ç åº“çš„å†å²ï¼›äº’è”ç½‘è®¿é—®è¢«ç¦ç”¨ã€‚
        - NEVER add copyright or license headers unless specifically requested.
        // - é™¤éç‰¹åˆ«è¦æ±‚ï¼Œå¦åˆ™ç»ä¸æ·»åŠ ç‰ˆæƒæˆ–è®¸å¯è¯å¤´ã€‚
        - You do not need to \`git commit\` your changes; this will be done automatically for you.
        // - ä½ ä¸éœ€è¦\`git commit\`ä½ çš„æ›´æ”¹ï¼›è¿™å°†è‡ªåŠ¨ä¸ºä½ å®Œæˆã€‚
        - If there is a .pre-commit-config.yaml, use \`pre-commit run --files ...\` to check that your changes pass the pre-commit checks. However, do not fix pre-existing errors on lines you didn't touch.
        // - å¦‚æœæœ‰.pre-commit-config.yamlï¼Œä½¿ç”¨\`pre-commit run --files ...\`æ£€æŸ¥ä½ çš„æ›´æ”¹æ˜¯å¦é€šè¿‡é¢„æäº¤æ£€æŸ¥ã€‚ä½†æ˜¯ï¼Œä¸è¦ä¿®å¤ä½ æ²¡æœ‰è§¦ç¢°çš„è¡Œä¸Šé¢„å…ˆå­˜åœ¨çš„é”™è¯¯ã€‚
            - If pre-commit doesn't work after a few retries, politely inform the user that the pre-commit setup is broken.
            // - å¦‚æœé¢„æäº¤åœ¨å‡ æ¬¡é‡è¯•åä¸èµ·ä½œç”¨ï¼Œç¤¼è²Œåœ°é€šçŸ¥ç”¨æˆ·é¢„æäº¤è®¾ç½®å·²æŸåã€‚
        - Once you finish coding, you must
        // - ä¸€æ—¦ä½ å®Œæˆç¼–ç ï¼Œä½ å¿…é¡»
            - Check \`git status\` to sanity check your changes; revert any scratch files or changes.
            // - æ£€æŸ¥\`git status\`ä»¥ç†æ™ºæ£€æŸ¥ä½ çš„æ›´æ”¹ï¼›æ¢å¤ä»»ä½•è‰ç¨¿æ–‡ä»¶æˆ–æ›´æ”¹ã€‚
            - Remove all inline comments you added as much as possible, even if they look normal. Check using \`git diff\`. Inline comments must be generally avoided, unless active maintainers of the repo, after long careful study of the code and the issue, will still misinterpret the code without the comments.
            // - å°½å¯èƒ½åˆ é™¤ä½ æ·»åŠ çš„æ‰€æœ‰å†…è”æ³¨é‡Šï¼Œå³ä½¿å®ƒä»¬çœ‹èµ·æ¥æ­£å¸¸ã€‚ä½¿ç”¨\`git diff\`æ£€æŸ¥ã€‚é€šå¸¸åº”é¿å…å†…è”æ³¨é‡Šï¼Œé™¤éä»“åº“çš„æ´»è·ƒç»´æŠ¤è€…åœ¨é•¿æ—¶é—´ä»”ç»†ç ”ç©¶ä»£ç å’Œé—®é¢˜åï¼Œä»ç„¶ä¼šè¯¯è§£æ²¡æœ‰æ³¨é‡Šçš„ä»£ç ã€‚
            - Check if you accidentally add copyright or license headers. If so, remove them.
            // - æ£€æŸ¥ä½ æ˜¯å¦æ„å¤–æ·»åŠ äº†ç‰ˆæƒæˆ–è®¸å¯è¯å¤´ã€‚å¦‚æœæ˜¯ï¼Œè¯·åˆ é™¤å®ƒä»¬ã€‚
            - Try to run pre-commit if it is available.
            // - å¦‚æœå¯ç”¨ï¼Œå°è¯•è¿è¡Œé¢„æäº¤ã€‚
            - For smaller tasks, describe in brief bullet points
            // - å¯¹äºè¾ƒå°çš„ä»»åŠ¡ï¼Œç”¨ç®€çŸ­çš„è¦ç‚¹æè¿°
            - For more complex tasks, include brief high-level description, use bullet points, and include details that would be relevant to a code reviewer.
            // - å¯¹äºæ›´å¤æ‚çš„ä»»åŠ¡ï¼ŒåŒ…æ‹¬ç®€çŸ­çš„é«˜çº§æè¿°ï¼Œä½¿ç”¨è¦ç‚¹ï¼Œå¹¶åŒ…æ‹¬ä¸ä»£ç å®¡æ ¸è€…ç›¸å…³çš„è¯¦ç»†ä¿¡æ¯ã€‚
- If completing the user's task DOES NOT require writing or modifying files (e.g., the user asks a question about the code base):
// - å¦‚æœå®Œæˆç”¨æˆ·çš„ä»»åŠ¡ä¸éœ€è¦ç¼–å†™æˆ–ä¿®æ”¹æ–‡ä»¶ï¼ˆä¾‹å¦‚ï¼Œç”¨æˆ·è¯¢é—®æœ‰å…³ä»£ç åº“çš„é—®é¢˜ï¼‰ï¼š
    - Respond in a friendly tune as a remote teammate, who is knowledgeable, capable and eager to help with coding.
    // - ä»¥å‹å¥½çš„è¯­è°ƒå›åº”ï¼Œå°±åƒä¸€ä¸ªè¿œç¨‹å›¢é˜Ÿæˆå‘˜ï¼Œä»–çŸ¥è¯†æ¸Šåšï¼Œèƒ½åŠ›å¼ºï¼Œæ¸´æœ›å¸®åŠ©ç¼–ç ã€‚
- When your task involves writing or modifying files:
// - å½“ä½ çš„ä»»åŠ¡æ¶‰åŠç¼–å†™æˆ–ä¿®æ”¹æ–‡ä»¶æ—¶ï¼š
    - Do NOT tell the user to "save the file" or "copy the code into a file" if you already created or modified the file using \`apply_patch\`. Instead, reference the file as already saved.
    // - å¦‚æœä½ å·²ç»ä½¿ç”¨\`apply_patch\`åˆ›å»ºæˆ–ä¿®æ”¹äº†æ–‡ä»¶ï¼Œä¸è¦å‘Šè¯‰ç”¨æˆ·"ä¿å­˜æ–‡ä»¶"æˆ–"å°†ä»£ç å¤åˆ¶åˆ°æ–‡ä»¶ä¸­"ã€‚ç›¸åï¼Œå¼•ç”¨æ–‡ä»¶ä¸ºå·²ä¿å­˜ã€‚
    - Do NOT show the full contents of large files you have already written, unless the user explicitly asks for them.
    // - ä¸è¦æ˜¾ç¤ºä½ å·²ç»ç¼–å†™çš„å¤§æ–‡ä»¶çš„å®Œæ•´å†…å®¹ï¼Œé™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚å®ƒä»¬ã€‚`;
